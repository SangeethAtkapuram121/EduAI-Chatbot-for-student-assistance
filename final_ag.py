import os
import json
import pickle
import tempfile
import speech_recognition as sr
import streamlit as st
import pytesseract
from PIL import Image
from typing import List, Tuple

from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.prompts.chat import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException
from huggingface_hub import login

# Ensure HuggingFace login
login(token="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")

# Set up Groq API key
os.environ["GROQ_API_KEY"] = "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"

# Initialize LLaMA model
@st.cache_resource
def initialize_llama_model():
    return ChatGroq(model_name="llama-3.3-70b-versatile", temperature=0)

# Initialize Gemma model using Google API
@st.cache_resource
def initialize_gemma_model():
    return ChatGroq(model_name="gemma2-9b-it", temperature=0)

# Load VectorStore dynamically based on user selection
@st.cache_resource
def load_vectorstore(index_dir: str, _embeddings) -> Tuple[FAISS, FAISS]:
    index_options = {
        0 : "./index_generated/Faculty_details/",
        1 : "./index_generated/Student_details/",
        2 : "./index_generated/Subjects/",
    }
    index1 = next((i for i, dir in index_options.items() if dir == index_dir), -1)
    file = f"index_files{index1}/"
    vectorstore = FAISS.load_local(index_dir+file, embeddings=_embeddings, allow_dangerous_deserialization=True)
    with open(f"{index_dir}/retriever.pkl", "rb") as f:
        retriever_store = pickle.load(f)
    return vectorstore, retriever_store

# OCR function to extract text from image
def extract_text_from_image(image: Image.Image) -> str:
    text = pytesseract.image_to_string(image)
    return text.strip()

# Process query with LLaMA
def process_query_with_llama(query: str, documents: List[str], messages: List[dict], llm) -> str:
    if not documents:
        return "No relevant documents found."
    
    messages.append({'role': 'user', 'content': query})
    history_str = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in messages])
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "Answer the user's query based on the provided documents in strict JSON format."),
        ("user", f"Previous conversation:\n{history_str}\nQuery: {{query}}\nDocuments:\n{{documents}}"),
        ("system", """
         Your response must be a valid JSON object.
         You should greet the user if they greet you.
         Improvise extracted content and return it as the answer.
         Format: {{"response": "Your answer here"}}.
         Strictly avoid any text outside the JSON object.
        """),
    ])
    
    parser = JsonOutputParser()
    chain = prompt_template | llm | parser

    try:
        result = chain.invoke({"query": query, "documents": "\n".join(documents), "messages": history_str})
        output = result.get("response", "Unable to generate a response.")
        messages.append({'role': 'assistant', 'content': output})
        return output
    except OutputParserException as e:
        st.error("Output parsing failed. Attempting to extract valid JSON...")
        llm_output = e.llm_output
        try:
            json_start = llm_output.find("{")
            json_end = llm_output.rfind("}") + 1
            valid_json = llm_output[json_start:json_end]
            parsed_output = json.loads(valid_json)
            return parsed_output.get("response", "Unable to extract a valid response.")
        except (json.JSONDecodeError, ValueError):
            st.error("Failed to parse JSON. Raw LLM output will be returned.")
            return f"Raw LLM Output:\n{llm_output}"

# Process query with Gemma model
def process_query_with_gemma(query: str, messages: List[dict], gemma_model) -> str:
    if not query:
        return "No query provided."
    
    messages.append({'role': 'user', 'content': query})
    history_str = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in messages])
    
    gemma_prompt_template = ChatPromptTemplate.from_messages([
        ("system", "Provide a concise and relevant answer to the user's query."),
        ("user", f"Previous conversation:\n{history_str}\nQuery: {{query}}"),
        ("system", """Ensure the response has quality and is under 300 tokens."""),
    ])

    prompt = gemma_prompt_template.format(query=query)
    try:
        response = gemma_model.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        if not response_text.strip():
            response_text = "I couldn't generate a response at the moment."
        
        messages.append({'role': 'assistant', 'content': response_text})
        return response_text
    
    except Exception as e:
        error_message = f"Error processing query: {str(e)}"
        messages.append({'role': 'assistant', 'content': error_message})
        return error_message

# Voice input function
def recognize_speech():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.info("Listening... Speak now.")
        try:
            audio = recognizer.listen(source, timeout=5)
            text = recognizer.recognize_google(audio)
            st.success(f"Recognized: {text}")
            return text
        except sr.UnknownValueError:
            st.warning("Could not understand audio.")
        except sr.RequestError:
            st.warning("Speech Recognition service is unavailable.")
        except sr.WaitTimeoutError:
            st.warning("No speech detected.")
    return ""

def clear_chat_history():
    st.session_state.messages = []

# Main Streamlit App
def main():
    st.set_page_config(page_title="EduAI: Chatbot for Student Assistance", layout="wide")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    st.sidebar.image("Mlrit.jpeg", use_column_width=True)
    
    index_options = {
        "Faculty": "./index_generated/Faculty_details/",
        "Students": "./index_generated/Student_details/",
        "Subjects": "./index_generated/Subjects/",
        "Subject_doubts": None
    }

    selected_index = st.sidebar.selectbox("Select Index Folder", list(index_options.keys()))
    uploaded_file = st.sidebar.file_uploader("Upload an image for OCR", type=["jpg", "png", "jpeg"])
    query = ""

    if st.sidebar.button("ðŸŽ¤ Use Voice Input", key="voice_input"):
        query = recognize_speech()
    
    st.sidebar.button("Clear Chat History", on_click=clear_chat_history)

    st.sidebar.header("Example Questions")
    for idx, question in enumerate(["How can I contact the faculty?", "Who is the HOD of CSE-AI&ML?"]):
        if st.sidebar.button(question, key=f"example_{idx}"):
            query = question
    
    st.title("EduAI: Chatbot for Student Assistance")
    st.subheader("Type your query below and get department-specific answers!")

    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as temp_file:
            temp_path = temp_file.name
            temp_file.write(uploaded_file.getbuffer())

        image = Image.open(temp_path)
        st.image(image, caption="Uploaded Image", use_column_width=True)

        pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
        extracted_text = extract_text_from_image(image)
        extracted_text += "\n\n Extract the information related to the above extracted data based on Roll No./Name"
        query = extracted_text if extracted_text else query
        image.close()
        os.remove(temp_path)

    user_input = st.chat_input("Enter your query:", key="user_query_input")
    query = user_input if user_input else query

    if query:
        if selected_index == "Subject_doubts":
            gemma_model = initialize_gemma_model()
            with st.spinner("Processing your query..."):
                process_query_with_gemma(query, st.session_state.messages, gemma_model)
                for message in st.session_state.messages:
                    with st.chat_message(message['role']):
                        st.markdown(message['content'])
        else:
            index_dir = index_options[selected_index]
            embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
            vectorstore, retriever_store = load_vectorstore(index_dir, embeddings)
            retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 20})
            results = retriever.get_relevant_documents(query)
            documents = [doc.page_content for doc in results]
            llm = initialize_llama_model()
            with st.spinner("Processing your query..."):
                process_query_with_llama(query, documents, st.session_state.messages, llm)
                for message in st.session_state.messages:
                    with st.chat_message(message['role']):
                        st.markdown(message['content'])

if __name__ == "__main__":
    main()
